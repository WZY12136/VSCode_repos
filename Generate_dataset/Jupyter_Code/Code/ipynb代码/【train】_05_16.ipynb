{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据导入__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import numpy as np\n",
    "\n",
    "# class Generate_Dataset(Dataset):\n",
    "#     def __init__(self, path):\n",
    "#         self.path = path\n",
    "#         self.data_path = glob.glob\\\n",
    "#             (os.path.join(path,'input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         data_path = self.data_path[index]\n",
    "#         data = np.load(data_path)      #读取输入数据\n",
    "#         tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "#         label_path = data_path.replace('input/', 'label/')\n",
    "#         label = np.load(label_path)    #读取标签数据\n",
    "#         tensor_label = torch.from_numpy(label)\n",
    "\n",
    "#         return tensor_data, tensor_label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data_path)\n",
    "# # if __name__ == '__main__':\n",
    "# #     dataset = Generate_Dataset('./dataset/')\n",
    "    \n",
    "# #     train_size = int(len(dataset) * 0.9)\n",
    "# #     validate_size = int(len(dataset) - train_size)\n",
    "# #     train_dataset, validate_dataset = torch.utils.data\\\n",
    "# #                 .random_split(dataset, [train_size, validate_size])\n",
    "\n",
    "# #     #print(\"读入数据个数为：\", len(top_dataset))\n",
    "# #     train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "# #     validate_loader = DataLoader(validate_dataset, batch_size=1, shuffle=True)\n",
    "# #     t = 0\n",
    "# #     for train, label in train_loader:\n",
    "# #         t += 1\n",
    "# #         print(train.shape)\n",
    "# #         print(label.shape)\n",
    "# #     print('共有',t,'个训练集')\n",
    "    \n",
    "# #     n = 0\n",
    "# #     for validate, label in validate_loader:\n",
    "# #         n += 1\n",
    "# #         print(validate.shape)\n",
    "# #         print(label.shape)\n",
    "# #     print('共有',n,'个验证集')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据集加载最新代码（negative_sample:positive_sample=7:3版本）__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class Generate_Dataset1(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'negative_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/negative_input', 'label/negative_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "class Generate_Dataset2(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'positive_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/positive_input', 'label/positive_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "# #测试载入数据程序\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     #读取并载入negative_dataset\n",
    "#     print('读入negative_sample...')\n",
    "#     print('***********************************************')\n",
    "#     dataset1 = Generate_Dataset1('./dataset_mini_batch/input/')\n",
    "#     negative_train_size = int(len(dataset1) * 0.7)\n",
    "#     print('negative_train_size:', negative_train_size)\n",
    "#     negative_validate_size = int(len(dataset1) - negative_train_size)\n",
    "#     print('negative_validate_size:', negative_validate_size)\n",
    "\n",
    "#     negative_train_dataset, negative_validate_dataset = torch.utils.data\\\n",
    "#                 .random_split(dataset1, [negative_train_size, negative_validate_size])\n",
    "\n",
    "#     print(\"读入数据个数为：\", (len(negative_train_dataset) + len(negative_validate_dataset)))\n",
    "#     print('***********************************************')\n",
    "#     print('读入完毕！')\n",
    "#     print('\\n')\n",
    "    \n",
    "#     #读取并载入positive_dataset\n",
    "#     print('读入positive_sample...')\n",
    "#     print('***********************************************')\n",
    "#     dataset2 = Generate_Dataset2('./dataset_mini_batch/input/')\n",
    "#     positive_train_size = int(len(dataset2) * 0.7)\n",
    "#     print('positive_train_size:',positive_train_size)\n",
    "#     positive_validate_size = int(len(dataset2) - positive_train_size)\n",
    "#     print('positive_validate_size:', positive_validate_size)\n",
    "#     positive_train_dataset, positive_validate_dataset = torch.utils.data\\\n",
    "#                 .random_split(dataset2, [positive_train_size, positive_validate_size])\n",
    "#     print(\"读入数据个数为：\", (len(positive_train_dataset) + len(positive_validate_dataset)))\n",
    "#     print('***********************************************')\n",
    "#     print('读入完毕！')\n",
    "    \n",
    "#     #正式加载数据集\n",
    "#     print('\\n')\n",
    "#     print('开始加载正式数据集...')\n",
    "#     print('***********************************************')\n",
    "#     train_dataset = torch.utils.data.ConcatDataset([negative_train_dataset, positive_train_dataset])\n",
    "#     validate_dataset = torch.utils.data.ConcatDataset([negative_validate_dataset, positive_validate_dataset])\n",
    "#     traindata_Loader = DataLoader(train_dataset, batch_size = 10, shuffle=True)\n",
    "#     validatedata_Loader = DataLoader(validate_dataset, batch_size = 10, shuffle=True)\n",
    "#     q = 0\n",
    "#     for train, label in traindata_Loader:\n",
    "#         print('train_shape:',train.shape)\n",
    "#         print('label_shape:',label.shape)\n",
    "#         q += 1\n",
    "#     print('测试集一共划分为：',q,'个批次')\n",
    "#     p = 0\n",
    "#     for validate, label in validatedata_Loader:\n",
    "#         print('v:',validate.shape)\n",
    "#         print('l:',label.shape)\n",
    "#         p += 1\n",
    "#     print('验证集一共划分为：',p,'个批次')\n",
    "#     print('***********************************************')\n",
    "#     print('正式数据集加载完毕!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__unet3d_parts__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3d_init(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv3d_init, self).__init__()\n",
    "        self.double_conv3d_init = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels=32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, out_channels, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.double_conv3d_init(input)\n",
    "\n",
    "\n",
    "class DoubleConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv3d, self).__init__()\n",
    "        self.double_conv3d = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, in_channels, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return self.double_conv3d(input)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down,self).__init__()\n",
    "        self.maxpool_conv3d = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2, padding=0),\n",
    "            DoubleConv3d(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.maxpool_conv3d(input)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up3d = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        self.conv = DoubleConv3d(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, input, x):  #x是接收的从encoder传过来的融合数据\n",
    "        # print('input',input.shape)\n",
    "        # print('x',x.shape)\n",
    "        x1 = self.up3d(input)\n",
    "        # print('x1',x1.shape)\n",
    "        diffY = torch.tensor(x.size()[3] - x1.size()[3])\n",
    "        diffX = torch.tensor(x.size()[4] - x1.size()[4])#特征融合部分\n",
    "        diffZ = torch.tensor(x.size()[2] - x1.size()[2])\n",
    "        #if x1.size()[3] > x.size()[3]:\n",
    "        x3 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2))\n",
    "        # print('x3',x3.shape)\n",
    "        output = torch.cat([x, x3], dim = 1)\n",
    "        return self.conv(output)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                    nn.Conv3d(in_channels, out_channels, kernel_size=(1, 1, 1)))\n",
    "    def forward(self, input):\n",
    "        return self.conv1(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__unet3d_model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self,in_channels, n_classes):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        #Encoder\n",
    "        self.inc = DoubleConv3d_init(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "\n",
    "        #Decoder\n",
    "        self.up1 = Up(768, 256)\n",
    "        self.up2 = Up(384, 128)\n",
    "        self.up3 = Up(192, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out1 = self.inc(input)\n",
    "        # print('out1.shape:',out1.shape)\n",
    "        out2 = self.down1(out1)\n",
    "        # print('out2.shape:',out2.shape)\n",
    "        out3 = self.down2(out2)\n",
    "        # print('out3.shape:',out3.shape)\n",
    "        out4 = self.down3(out3)\n",
    "        # print('out4.shape:',out4.shape)\n",
    "        out5 = self.up1(out4, out3)\n",
    "        # print('out5.shape:',out5.shape)\n",
    "        out6 = self.up2(out5, out2)\n",
    "        # print('out6.shape:',out6.shape)\n",
    "        out7 = self.up3(out6, out1)\n",
    "        # print('out7.shape:',out7.shape)\n",
    "        logits = self.outc(out7)\n",
    "        # print('logits.shape:',logits.shape)\n",
    "        return logits\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     net = UNet3D(in_channels =3 ,n_classes=3)\n",
    "#     print(net)\n",
    "#     para = list(net.parameters())\n",
    "#     print('parameters:', para)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./traindata_record')\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#类平衡损失函数\n",
    "def cb_loss(y_pred, y_true, temp, beta, confusion):\n",
    "\n",
    "    #计算每个类别对应的样本数\n",
    "    confusion_ = [torch.sum(temp == i) for i in range(2)]   #统计一张label中每个类别的像素点\n",
    "    # print('calsse_number:',i)\n",
    "    # print('confusion_:',confusion_)\n",
    "    weight = []\n",
    "    weight_dice = []\n",
    "    for i, n in zip(confusion_, confusion):\n",
    "        if i == 0:\n",
    "            weight.append(0)\n",
    "            weight_dice.append(1)\n",
    "        else:\n",
    "            weight_dice.append(1)\n",
    "            weight.append(((1.0 - beta) / (1.0 - math.pow(beta, i))))\n",
    "    \n",
    "    \n",
    "    weight = torch.FloatTensor(weight)\n",
    "    # print(weight)\n",
    "    weight_dice = torch.FloatTensor(weight_dice)\n",
    "    weight = weight.to(y_pred.device)\n",
    "    weight_dice = weight_dice.to(y_pred.device)\n",
    "\n",
    "    # print(y_pred.size(), y_true.size())\n",
    "    criterion_train = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "    loss = criterion_train(y_pred.float(), temp.long())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#生成混淆矩阵\n",
    "@torch.no_grad()\n",
    "def get_confusion_matrix(trues, preds):\n",
    "    labels = [0, 1]\n",
    "    conf_matrix = confusion_matrix(trues, preds, labels = labels)\n",
    "    return conf_matrix\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_confusion_matrix(conf_matrix):\n",
    "    plt.imshow(conf_matrix, cmap=plt.cm.Greens)\n",
    "    indices = range(conf_matrix.shape[0])\n",
    "    labels = [0, 1]\n",
    "    plt.xticks(indices, labels)\n",
    "    plt.yticks(indices, labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('y_pred')\n",
    "    plt.ylabel('y_true')\n",
    "\n",
    "    for first_index in range(conf_matrix.shape[0]):\n",
    "        for second_index in range(conf_matrix.shape[1]):\n",
    "            plt.text(second_index, first_index, conf_matrix[first_index, second_index])\n",
    "    plt.savefig('confusion_matrix.jpg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_train(net_c ,data_loader, epoch, confusion):\n",
    "    # netc = net_c.to(device=torch.device('cpu'))\n",
    "    validateloss = 0\n",
    "    for validatedata, validatelabel in data_loader:\n",
    "        validatedata = validatedata.to(device=torch.device('cuda'), dtype=torch.float32)\n",
    "        validatelabel = validatelabel.to(device=torch.device('cuda'), dtype=torch.float32)\n",
    "        validatepred = net_c(validatedata)\n",
    "        validatetemp = validatelabel.squeeze(1).to(device=torch.device('cuda'))\n",
    "        validateloss += cb_loss(validatepred, validatelabel, validatetemp, 0.99, confusion).item()\n",
    "    validateloss = validateloss / (len(data_loader.dataset) / 50)\n",
    "    writer.add_scalar('validateloss',float(validateloss),epoch)\n",
    "    writer.close()\n",
    "    print(\"-\"*60+\"\\n [validate_Epoch]:{},\\n [validate_Loss]:{}\".format(epoch, validateloss))\n",
    "\n",
    "\n",
    "def train_net(net, device, data_path, epochs=500, batch_size=10, lr=0.000001):\n",
    "\n",
    "    #设置dataloader固定生成器\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "    #读取并载入negative_dataset\n",
    "    dataset1 = Generate_Dataset1(data_path)\n",
    "    negative_train_size = int(len(dataset1) * 0.7)\n",
    "    \n",
    "    negative_validate_size = int(len(dataset1) - negative_train_size)\n",
    "    negative_train_dataset, negative_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset1, [negative_train_size, negative_validate_size],generator = generator)\n",
    "    \n",
    "    #读取并载入positive_dataset\n",
    "    dataset2 = Generate_Dataset2(data_path)\n",
    "    positive_train_size = int(len(dataset2) * 0.7)\n",
    "    \n",
    "    positive_validate_size = int(len(dataset2) - positive_train_size)\n",
    "    positive_train_dataset, positive_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset2, [positive_train_size, positive_validate_size], generator = generator)\n",
    "\n",
    "    #数据集按:negative:positive=7:3比例进行合并\n",
    "    train_dataset = torch.utils.data.ConcatDataset([negative_train_dataset, positive_train_dataset])\n",
    "    # print('train_dataset_num:',len(train_dataset))\n",
    "    validate_dataset = torch.utils.data.ConcatDataset([negative_validate_dataset, positive_validate_dataset])\n",
    "    # print('validate_dataset_num:',len(validate_dataset))\n",
    "    traindata_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    # print('traindata_loader:',len(traindata_loader))\n",
    "    # validatedata_loader = DataLoader(validate_dataset, len(validate_dataset), shuffle=True)\n",
    "    validatedata_loader = DataLoader(validate_dataset, batch_size, shuffle=True)\n",
    "    # print('validatedata_loader:',len(validatedata_loader))\n",
    "    #设置Adam优化器\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.0000002)\n",
    "    #设置损失函数\n",
    "    # crossentropyloss = torch.nn.BCEWithLogitsLoss()\n",
    "    #设置终止迭代loss阈值\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    #设置损失函数相关参数\n",
    "    confusion = [1, 1]   #number of negative sample:6733/number of positive sample:222\n",
    "\n",
    "\n",
    "    #开始训练\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        train_preds = []\n",
    "        train_trues = []\n",
    "        #训练数据集\n",
    "       \n",
    "        for train_data, train_label in (traindata_loader):\n",
    "        \n",
    "            train_data = train_data.to(device=device, dtype=torch.float32)\n",
    "            train_label = train_label.to(device=device, dtype=torch.float32)\n",
    "            \n",
    "            pred = net(train_data)\n",
    "            temp = train_label.squeeze(1).to(device)\n",
    "            loss = cb_loss(pred, train_label, temp, 0.9999, confusion)\n",
    "            writer.add_scalar('trainloss',float(loss),epoch)\n",
    "            writer.close()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            #评价指标计算\n",
    "            train_outs = pred.argmax(dim=1)\n",
    "            train_preds.extend(train_outs.detach().cpu().numpy())\n",
    "            train_trues.extend(train_label.detach().cpu().numpy())\n",
    "            train_pred = np.array(train_preds).reshape(-1,1)\n",
    "            train_true = np.array(train_trues).reshape(-1,1)\n",
    "\n",
    "            sklearn_accuracy = accuracy_score(train_true, train_pred)\n",
    "            writer.add_scalar('accuracy_score',float(sklearn_accuracy),epoch)\n",
    "            writer.close()\n",
    "            sklearn_precision = precision_score(train_true, train_pred)\n",
    "            writer.add_scalar('precision_score',float(sklearn_precision),epoch)\n",
    "            writer.close()\n",
    "            sklearn_recall = recall_score(train_true, train_pred)\n",
    "            writer.add_scalar('recall_score',float(sklearn_recall),epoch)\n",
    "            writer.close()\n",
    "            sklearn_f1 = f1_score(train_true, train_pred)\n",
    "            writer.add_scalar('f1_score',float(sklearn_f1),epoch)\n",
    "            writer.close()\n",
    "            \n",
    "            print(\"[sklearn_metrics]  Epoch:{} \\\n",
    "                  \\n [Train_loss]:{} \\n [accuracy]:{:.4f} \\n [precision]:{:4f} \\n [recall]:{:4f} \\n [f1]:{:4f}\".format\\\n",
    "                  (epoch, loss, sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1))\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            torch.save(net.state_dict(), 'best_model.pth')\n",
    "            \n",
    "            #验证集测试\n",
    "            net.eval()\n",
    "            validation_train(net ,validatedata_loader, epoch, confusion)\n",
    "            print('='*120)\n",
    "        scheduler.step()\n",
    "        \n",
    "        #混淆矩阵\n",
    "        conf_matrix = get_confusion_matrix(train_true, train_pred)\n",
    "        plot_confusion_matrix(conf_matrix)\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = UNet3D(1, 2)\n",
    "    net.to(device=device)\n",
    "\n",
    "    data_path = './dataset_mini_batch/input/'\n",
    "    train_net(net, device, data_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 选择设备，有cuda用cuda，没有就用cpu\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     # 加载网络，图片单通道，分类为1。\n",
    "#     net = UNet3D(1, 1)\n",
    "#     # 将网络拷贝到deivce中\n",
    "#     net.to(device=device)\n",
    "#     # 加载模型参数\n",
    "#     net.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "#     # 测试模式\n",
    "#     net.eval()\n",
    "#     t = 50\n",
    "#     # 保存结果地址\n",
    "#     save_res_path = os.path.join('./dataset/test/'+('%d_res.png'%(t)))\n",
    "#     # 转为batch为1，通道为1，大小为512*512的数组\n",
    "#     data = np.load('./dataset/test/topomodel_50.npy')\n",
    "#     data = data.reshape(1, 1, data.shape[0], data.shape[1], data.shape[2])\n",
    "#     # 转为tensor\n",
    "#     data_tensor = torch.from_numpy(data)\n",
    "#     # 将tensor拷贝到device中，只用cpu就是拷贝到cpu中，用cuda就是拷贝到cuda中。\n",
    "#     data_tensor = data_tensor.to(device=device, dtype=torch.float32)\n",
    "#     # 预测\n",
    "#     pred = net(data_tensor)\n",
    "#     # 提取结果\n",
    "#     pred = np.array(pred.data.cpu()[0])[0]\n",
    "#     print(pred)\n",
    "#     # # 处理结果\n",
    "#     # for i in range(pred.shape[0]):\n",
    "#     #     for j in range(pred.shape[1]):\n",
    "#     #         for k in range(pred.shape[2]):\n",
    "#     #             if pred[i][j][k] > 0.5:\n",
    "#     #                 pred[i][j][k] = 1\n",
    "#     #             else:\n",
    "#     #                 pred[i][j][k] = 0\n",
    "#     # 保存图片\n",
    "#     np.save('./dataset/test/',pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topopt",
   "language": "python",
   "name": "topopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
