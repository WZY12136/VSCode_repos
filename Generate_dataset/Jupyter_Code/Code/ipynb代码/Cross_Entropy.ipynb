{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CrossEntropyLoss详解__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " tensor([[-0.5807,  0.3774,  0.6274, -1.1844,  1.1816],\n",
      "        [-0.3242,  0.9124, -0.4214, -0.3013, -1.3511],\n",
      "        [-1.2594, -0.4036, -0.3842, -0.5969, -1.3031],\n",
      "        [ 0.9459,  2.5789, -1.5776,  0.5863, -0.4910]])\n",
      "target:\n",
      " tensor([2, 1, 3, 4])\n",
      "one_hot:\n",
      " tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "softmax:\n",
      " tensor([[0.0750, 0.1956, 0.2512, 0.0410, 0.4371],\n",
      "        [0.1485, 0.5115, 0.1348, 0.1520, 0.0532],\n",
      "        [0.1156, 0.2721, 0.2774, 0.2242, 0.1107],\n",
      "        [0.1402, 0.7175, 0.0112, 0.0978, 0.0333]])\n",
      "logsoftmax:\n",
      " tensor([[-2.5897, -1.6316, -1.3816, -3.1935, -0.8275],\n",
      "        [-1.9069, -0.6704, -2.0042, -1.8840, -2.9338],\n",
      "        [-2.1575, -1.3017, -1.2823, -1.4950, -2.2012],\n",
      "        [-1.9650, -0.3320, -4.4885, -2.3246, -3.4019]])\n",
      "one_hot*logsoftmax:\n",
      " tensor([[-0.0000, -0.0000, -1.3816, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.6704, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -1.4950, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, -3.4019]])\n",
      "a torch.Size([4, 5])\n",
      "nllloss:\n",
      " tensor(1.7372)\n",
      "loss:\n",
      " tensor(1.7372)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#生成神经网络output，一行对应一个样本，一列对应该样本属于该列类的预测概率\n",
    "output = torch.randn(4, 5)\n",
    "print('output:\\n',output)\n",
    "\n",
    "#生成5个样本的标签，并转换为One-Hot编码\n",
    "target = torch.tensor([2, 1, 3, 4])\n",
    "print('target:\\n',target)\n",
    "one_hot = F.one_hot(target).float()\n",
    "print('one_hot:\\n',one_hot)\n",
    "\n",
    "#softmax层运算，每个通道每个像素点的值转换为预测概率值\n",
    "softmax = torch.exp(output)/torch.sum(torch.exp(output),dim=1).reshape(-1, 1)\n",
    "print('softmax:\\n',softmax)\n",
    "\n",
    "#对数运算，保证数值稳定性和overflaw、underflaw\n",
    "logsoftmax = torch.log(softmax)\n",
    "print('logsoftmax:\\n',logsoftmax)\n",
    "\n",
    "#按照label标号为1的像素点位置读取logsoftmax，计算损失函数值\n",
    "nllloss = -torch.sum(one_hot*logsoftmax)/target.shape[0]\n",
    "print('one_hot*logsoftmax:\\n',one_hot*logsoftmax)\n",
    "a = one_hot*logsoftmax\n",
    "print('a',a.shape)\n",
    "print('nllloss:\\n',nllloss)\n",
    "\n",
    "#直接使用封装好的CrossEntropyLoss计算损失函数值\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "value = loss(output, target)\n",
    "print('loss:\\n',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 2, 0],\n",
      "        [1, 1, 4, 1],\n",
      "        [0, 3, 4, 2],\n",
      "        [1, 4, 0, 4]])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4, 5])\n",
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "target = torch.randint(0, 5, (4, 4))\n",
    "print(target)\n",
    "print(target.shape)\n",
    "one_hot = F.one_hot(target).float()\n",
    "print(one_hot.shape)\n",
    "print(one_hot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__咽喉反流CB_Loss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "def cb_loss(y_pred, y_true, temp, beta, confusion):\n",
    "    confusion_ = [torch.sum(temp == i) for i in range(8)]   #统计一张label中每个类别的像素点\n",
    "\n",
    "    weight = []\n",
    "    weight_dice = []\n",
    "    for i, n in zip(confusion_, confusion):\n",
    "        if i == 0:\n",
    "            weight.append(0)\n",
    "            weight_dice.append(1)\n",
    "        else:\n",
    "            weight_dice.append(1)\n",
    "            weight.append(((1.0 - beta) / (1.0 - math.pow(beta, n))))\n",
    "    \n",
    "    \n",
    "    weight = torch.FloatTensor(weight)\n",
    "    weight_dice = torch.FloatTensor(weight_dice)\n",
    "    weight = weight.to(y_pred.device)\n",
    "    weight_dice = weight_dice.to(y_pred.device)\n",
    "\n",
    "\n",
    "    criterion_train = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    loss = criterion_train(y_pred.float(), temp.float())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CB_Loss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def cb_loss(y_pred, y_true, temp, beta, confusion):\n",
    "    \n",
    "    \n",
    "    #统计标签MASK中每个类的样本数量，temp=y_true\n",
    "    confusion_ = [torch.sum(temp == i) for i in range(2)]\n",
    "\n",
    "    weight = []\n",
    "    weight_dice = []\n",
    "    for i, n in zip(confusion_, confusion):\n",
    "        if i == 0:\n",
    "            weight.append(0)\n",
    "            weight_dice.append(1)\n",
    "        else:\n",
    "            weight_dice.append(1)\n",
    "            weight.append(((1.0 - beta) / (1.0 - math.pow(beta, n))))\n",
    "    \n",
    "    weight = torch.FloatTensor(weight)\n",
    "    weight_dice = torch.FloatTensor(weight_dice)\n",
    "    weight = weight.to(y_pred.device)\n",
    "    weight_dice = weight_dice.to(y_pred.device)\n",
    "\n",
    "\n",
    "    criterion_train = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    loss = criterion_train(y_pred.float(), temp.float())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 3, 1, 2],\n",
      "        [0, 2, 1, 3, 3],\n",
      "        [1, 2, 1, 1, 2],\n",
      "        [2, 0, 3, 0, 0],\n",
      "        [1, 0, 3, 1, 4]])\n",
      "[tensor(5), tensor(7), tensor(6), tensor(6), tensor(1)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "target = torch.randint(0, 5, (5, 5))\n",
    "print(target)\n",
    "confusion_ = [torch.sum(target == i) for i in range(5)]\n",
    "print(confusion_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据集加载最新版本（negative:positive=7:3）（代码保留）__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读入negative_sample...\n",
      "***********************************************\n",
      "negative_train_size: 6733\n",
      "negative_validate_size: 2886\n",
      "读入数据个数为： 9619\n",
      "***********************************************\n",
      "读入完毕！\n",
      "\n",
      "\n",
      "读入positive_sample...\n",
      "***********************************************\n",
      "positive_train_size: 222\n",
      "positive_validate_size: 95\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "array() takes from 1 to 2 positional arguments but 222 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 107\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpositive_validate_size:\u001b[39m\u001b[39m'\u001b[39m, positive_validate_size)\n\u001b[0;32m    102\u001b[0m positive_train_dataset, positive_validate_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\\\n\u001b[0;32m    103\u001b[0m             \u001b[39m.\u001b[39mrandom_split(dataset2, [positive_train_size, positive_validate_size], generator \u001b[39m=\u001b[39m generator)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpositive_train_dataset:\u001b[39m\u001b[39m'\u001b[39m,np\u001b[39m.\u001b[39;49marray(\u001b[39m*\u001b[39;49mpositive_train_dataset\u001b[39m.\u001b[39;49mindices)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    112\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m读入数据个数为：\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(positive_train_dataset)\u001b[39m+\u001b[39m\u001b[39mlen\u001b[39m(positive_validate_dataset))\n\u001b[0;32m    113\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m***********************************************\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: array() takes from 1 to 2 positional arguments but 222 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class Generate_Dataset1(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'negative_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/negative_input', 'label/negative_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "class Generate_Dataset2(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'positive_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/positive_input', 'label/positive_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "#测试载入数据程序\n",
    "if __name__ == '__main__':\n",
    "    #设置dataloader固定生成器\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    " \n",
    "    #读取并载入negative_dataset\n",
    "    print('读入negative_sample...')\n",
    "    print('***********************************************')\n",
    "    dataset1 = Generate_Dataset1('../Data/dataset(positive_negative)/input/')\n",
    "    negative_train_size = int(len(dataset1) * 0.7)\n",
    "    print('negative_train_size:', negative_train_size)\n",
    "    negative_validate_size = int(len(dataset1) - negative_train_size)\n",
    "    print('negative_validate_size:', negative_validate_size)\n",
    "\n",
    "    negative_train_dataset, negative_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset1, [negative_train_size, negative_validate_size], generator = generator)\n",
    "\n",
    "    print(\"读入数据个数为：\", len(negative_train_dataset)+len(negative_validate_dataset))\n",
    "    print('***********************************************')\n",
    "    print('读入完毕！')\n",
    "    print('\\n')\n",
    "    # train_loader = DataLoader(negative_train_dataset, batch_size=1, shuffle=True)\n",
    "    # validate_loader = DataLoader(negative_validate_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # t = 0\n",
    "    # for negative_train, negative_label in train_loader:\n",
    "    #     t += 1\n",
    "    #     print(negative_train.shape)\n",
    "    #     print(negative_label.shape)\n",
    "    # print('共有',t,'个训练集')\n",
    "    \n",
    "    # n = 0\n",
    "    # for negative_validate, negative_label in validate_loader:\n",
    "    #     n += 1\n",
    "    #     print(negative_validate.shape)\n",
    "    #     print(negative_label.shape)\n",
    "    # print('共有',n,'个训练集')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #读取并载入positive_dataset\n",
    "    print('读入positive_sample...')\n",
    "    print('***********************************************')\n",
    "    \n",
    "    dataset2 = Generate_Dataset2('../Data/dataset(positive_negative)/input/')\n",
    "    positive_train_size = int(len(dataset2) * 0.7) + 1\n",
    "    print('positive_train_size:',positive_train_size)\n",
    "    positive_validate_size = int(len(dataset2) - positive_train_size)\n",
    "    print('positive_validate_size:', positive_validate_size)\n",
    "    positive_train_dataset, positive_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset2, [positive_train_size, positive_validate_size], generator = generator)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('positive_train_dataset:',np.array(*positive_train_dataset.indices).shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"读入数据个数为：\", len(positive_train_dataset)+len(positive_validate_dataset))\n",
    "    print('***********************************************')\n",
    "    print('读入完毕！')\n",
    "    # train_loader = DataLoader(positive_train_dataset, batch_size=1, shuffle=True)\n",
    "    # validate_loader = DataLoader(positive_validate_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # t = 0\n",
    "    # for positive_train, positive_label in train_loader:\n",
    "    #     t += 1\n",
    "    #     # print(positive_train.shape)\n",
    "    #     # print(positive_label.shape)\n",
    "    # # print('共有',t,'个训练集')\n",
    "    \n",
    "    # n = 0\n",
    "    # for positive_validate, positive_label in validate_loader:\n",
    "    #     n += 1\n",
    "    #     # print(positive_validate.shape)\n",
    "    #     # print(positive_label.shape)\n",
    "    # # print('共有',n,'个训练集')\n",
    "\n",
    "    train_dataset = torch.utils.data.ConcatDataset([negative_train_dataset, positive_train_dataset])\n",
    "    validate_dataset = torch.utils.data.ConcatDataset([negative_validate_dataset, positive_validate_dataset])\n",
    "    print('\\n')\n",
    "    print('train_dataset/validate_dataset数量统计...')\n",
    "    print('***********************************************')\n",
    "    print('Number of train_dataset:',len(train_dataset))\n",
    "    print('Number of validate_dataset:',len(validate_dataset))\n",
    "    print('***********************************************')\n",
    "    # print(len(train_dataset))\n",
    "    # print(len(validate_dataset))\n",
    "    # print(list(train_dataset))\n",
    "    # m = 0\n",
    "    # for data, label in validate_dataset:\n",
    "    #     m += 1\n",
    "    #     # print(data)\n",
    "    #     print('******************************************************************')\n",
    "    #     # print(label)\n",
    "    #     print('一共',m,'个数据')\n",
    "\n",
    "    print('\\n')\n",
    "    print('开始加载正式数据集...')\n",
    "    print('***********************************************')\n",
    "    traindata_Loader = DataLoader(train_dataset, batch_size = 50, shuffle=True)\n",
    "    validatedata_Loader = DataLoader(validate_dataset, batch_size = 50, shuffle=True)\n",
    "    q = 0\n",
    "    for train, label in traindata_Loader:\n",
    "        # print(train.shape)\n",
    "        # print(label.shape)\n",
    "        q += 1\n",
    "    print('测试集一共划分为：',q,'个批次')\n",
    "    p = 0\n",
    "    for validate, label in validatedata_Loader:\n",
    "        # print(validate.shape)\n",
    "        # print(label.shape)\n",
    "        p += 1\n",
    "    print('验证集一共划分为：',p,'个批次')\n",
    "    print('***********************************************')\n",
    "    print('正式数据集加载完毕！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "pytorch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
