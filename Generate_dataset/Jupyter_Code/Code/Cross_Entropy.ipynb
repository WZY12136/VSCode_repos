{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CrossEntropyLoss详解__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " tensor([[-0.0106, -0.4310, -1.6414,  1.2071,  0.3368],\n",
      "        [-0.2920,  1.3862, -1.0200, -0.1568, -1.7095],\n",
      "        [-0.0953,  0.6084, -0.6550,  0.4059,  0.7989],\n",
      "        [-1.2834,  0.6305,  0.5013,  1.0222, -0.6259],\n",
      "        [ 2.5351,  1.5265,  0.5361, -1.2505,  1.2854]])\n",
      "target:\n",
      " tensor([0, 2, 1, 3, 4])\n",
      "one_hot:\n",
      " tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "softmax:\n",
      " tensor([[0.1504, 0.0988, 0.0295, 0.5084, 0.2129],\n",
      "        [0.1216, 0.6511, 0.0587, 0.1392, 0.0295],\n",
      "        [0.1301, 0.2629, 0.0743, 0.2147, 0.3180],\n",
      "        [0.0389, 0.2638, 0.2318, 0.3903, 0.0751],\n",
      "        [0.5526, 0.2016, 0.0749, 0.0125, 0.1584]])\n",
      "logsoftmax:\n",
      " tensor([[-1.8943, -2.3146, -3.5250, -0.6765, -1.5468],\n",
      "        [-2.1073, -0.4291, -2.8353, -1.9721, -3.5248],\n",
      "        [-2.0397, -1.3360, -2.5994, -1.5386, -1.1456],\n",
      "        [-3.2464, -1.3325, -1.4617, -0.9408, -2.5889],\n",
      "        [-0.5930, -1.6017, -2.5920, -4.3786, -1.8428]])\n",
      "one_hot*logsoftmax:\n",
      " tensor([[-1.8943, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -2.8353, -0.0000, -0.0000],\n",
      "        [-0.0000, -1.3360, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.9408, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000, -1.8428]])\n",
      "nllloss:\n",
      " tensor(1.7698)\n",
      "loss:\n",
      " tensor(1.7698)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#生成神经网络output，一行对应一个样本，一列对应该样本属于该列类的预测概率\n",
    "output = torch.randn(5, 5)\n",
    "print('output:\\n',output)\n",
    "\n",
    "#生成5个样本的标签，并转换为One-Hot编码\n",
    "target = torch.tensor([0, 2, 1, 3, 4])\n",
    "print('target:\\n',target)\n",
    "one_hot = F.one_hot(target).float()\n",
    "print('one_hot:\\n',one_hot)\n",
    "\n",
    "#softmax层运算，每个通道每个像素点的值转换为预测概率值\n",
    "softmax = torch.exp(output)/torch.sum(torch.exp(output),dim=1).reshape(-1, 1)\n",
    "print('softmax:\\n',softmax)\n",
    "\n",
    "#对数运算，保证数值稳定性和overflaw、underflaw\n",
    "logsoftmax = torch.log(softmax)\n",
    "print('logsoftmax:\\n',logsoftmax)\n",
    "\n",
    "#按照label标号为1的像素点位置读取logsoftmax，计算损失函数值\n",
    "nllloss = -torch.sum(one_hot*logsoftmax)/target.shape[0]\n",
    "print('one_hot*logsoftmax:\\n',one_hot*logsoftmax)\n",
    "print('nllloss:\\n',nllloss)\n",
    "\n",
    "#直接使用封装好的CrossEntropyLoss计算损失函数值\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "value = loss(output, target)\n",
    "print('loss:\\n',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 2, 0],\n",
      "        [1, 1, 4, 1],\n",
      "        [0, 3, 4, 2],\n",
      "        [1, 4, 0, 4]])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4, 5])\n",
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "target = torch.randint(0, 5, (4, 4))\n",
    "print(target)\n",
    "print(target.shape)\n",
    "one_hot = F.one_hot(target).float()\n",
    "print(one_hot.shape)\n",
    "print(one_hot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__咽喉反流CB_Loss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "def cb_loss(y_pred, y_true, temp, beta, confusion):\n",
    "    confusion_ = [torch.sum(temp == i) for i in range(8)]   #统计一张label中每个类别的像素点\n",
    "\n",
    "    weight = []\n",
    "    weight_dice = []\n",
    "    for i, n in zip(confusion_, confusion):\n",
    "        if i == 0:\n",
    "            weight.append(0)\n",
    "            weight_dice.append(1)\n",
    "        else:\n",
    "            weight_dice.append(1)\n",
    "            weight.append(((1.0 - beta) / (1.0 - math.pow(beta, n))))\n",
    "    \n",
    "    \n",
    "    weight = torch.FloatTensor(weight)\n",
    "    weight_dice = torch.FloatTensor(weight_dice)\n",
    "    weight = weight.to(y_pred.device)\n",
    "    weight_dice = weight_dice.to(y_pred.device)\n",
    "\n",
    "\n",
    "    criterion_train = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    loss = criterion_train(y_pred.float(), temp.float())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CB_Loss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def cb_loss(y_pred, y_true, temp, beta, confusion):\n",
    "    \n",
    "    \n",
    "    #统计标签MASK中每个类的样本数量，temp=y_true\n",
    "    confusion_ = [torch.sum(temp == i) for i in range(2)]\n",
    "\n",
    "    weight = []\n",
    "    weight_dice = []\n",
    "    for i, n in zip(confusion_, confusion):\n",
    "        if i == 0:\n",
    "            weight.append(0)\n",
    "            weight_dice.append(1)\n",
    "        else:\n",
    "            weight_dice.append(1)\n",
    "            weight.append(((1.0 - beta) / (1.0 - math.pow(beta, n))))\n",
    "    \n",
    "    weight = torch.FloatTensor(weight)\n",
    "    weight_dice = torch.FloatTensor(weight_dice)\n",
    "    weight = weight.to(y_pred.device)\n",
    "    weight_dice = weight_dice.to(y_pred.device)\n",
    "\n",
    "\n",
    "    criterion_train = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    loss = criterion_train(y_pred.float(), temp.float())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 3, 1, 2],\n",
      "        [0, 2, 1, 3, 3],\n",
      "        [1, 2, 1, 1, 2],\n",
      "        [2, 0, 3, 0, 0],\n",
      "        [1, 0, 3, 1, 4]])\n",
      "[tensor(5), tensor(7), tensor(6), tensor(6), tensor(1)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "target = torch.randint(0, 5, (5, 5))\n",
    "print(target)\n",
    "confusion_ = [torch.sum(target == i) for i in range(5)]\n",
    "print(confusion_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据集加载最新版本（negative:positive=7:3）（代码保留）__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读入negative_sample...\n",
      "***********************************************\n",
      "negative_train_size: 6733\n",
      "negative_validate_size: 2886\n",
      "读入数据个数为： 6733\n",
      "***********************************************\n",
      "读入完毕！\n",
      "\n",
      "\n",
      "读入positive_sample...\n",
      "***********************************************\n",
      "positive_train_size: 222\n",
      "positive_validate_size: 95\n",
      "读入数据个数为： 222\n",
      "***********************************************\n",
      "读入完毕！\n",
      "\n",
      "\n",
      "开始加载正式数据集...\n",
      "***********************************************\n",
      "测试集一共划分为： 140 个批次\n",
      "验证集一共划分为： 60 个批次\n",
      "***********************************************\n",
      "正式数据集加载完毕！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class Generate_Dataset1(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'negative_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/negative_input', 'label/negative_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "class Generate_Dataset2(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'positive_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/positive_input', 'label/positive_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "#测试载入数据程序\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    " \n",
    "    #读取并载入negative_dataset\n",
    "    print('读入negative_sample...')\n",
    "    print('***********************************************')\n",
    "    dataset1 = Generate_Dataset1('../Data/dataset(positive_negative)/input/')\n",
    "    negative_train_size = int(len(dataset1) * 0.7)\n",
    "    print('negative_train_size:', negative_train_size)\n",
    "    negative_validate_size = int(len(dataset1) - negative_train_size)\n",
    "    print('negative_validate_size:', negative_validate_size)\n",
    "\n",
    "    negative_train_dataset, negative_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset1, [negative_train_size, negative_validate_size])\n",
    "\n",
    "    print(\"读入数据个数为：\", len(negative_train_dataset))\n",
    "    print('***********************************************')\n",
    "    print('读入完毕！')\n",
    "    print('\\n')\n",
    "    train_loader = DataLoader(negative_train_dataset, batch_size=1, shuffle=True)\n",
    "    validate_loader = DataLoader(negative_validate_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # t = 0\n",
    "    # for negative_train, negative_label in train_loader:\n",
    "    #     t += 1\n",
    "    #     print(negative_train.shape)\n",
    "    #     print(negative_label.shape)\n",
    "    # print('共有',t,'个训练集')\n",
    "    \n",
    "    # n = 0\n",
    "    # for negative_validate, negative_label in validate_loader:\n",
    "    #     n += 1\n",
    "    #     print(negative_validate.shape)\n",
    "    #     print(negative_label.shape)\n",
    "    # print('共有',n,'个训练集')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #读取并载入positive_dataset\n",
    "    print('读入positive_sample...')\n",
    "    print('***********************************************')\n",
    "    \n",
    "    dataset2 = Generate_Dataset2('../Data/dataset(positive_negative)/input/')\n",
    "    positive_train_size = int(len(dataset2) * 0.7) + 1\n",
    "    print('positive_train_size:',positive_train_size)\n",
    "    positive_validate_size = int(len(dataset2) - positive_train_size)\n",
    "    print('positive_validate_size:', positive_validate_size)\n",
    "    positive_train_dataset, positive_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset2, [positive_train_size, positive_validate_size])\n",
    "\n",
    "    print(\"读入数据个数为：\", len(positive_train_dataset))\n",
    "    print('***********************************************')\n",
    "    print('读入完毕！')\n",
    "    train_loader = DataLoader(positive_train_dataset, batch_size=1, shuffle=True)\n",
    "    validate_loader = DataLoader(positive_validate_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # t = 0\n",
    "    # for positive_train, positive_label in train_loader:\n",
    "    #     t += 1\n",
    "    #     # print(positive_train.shape)\n",
    "    #     # print(positive_label.shape)\n",
    "    # # print('共有',t,'个训练集')\n",
    "    \n",
    "    # n = 0\n",
    "    # for positive_validate, positive_label in validate_loader:\n",
    "    #     n += 1\n",
    "    #     # print(positive_validate.shape)\n",
    "    #     # print(positive_label.shape)\n",
    "    # # print('共有',n,'个训练集')\n",
    "    print('\\n')\n",
    "    print('开始加载正式数据集...')\n",
    "    print('***********************************************')\n",
    "    \n",
    "    train_dataset = torch.utils.data.ConcatDataset([negative_train_dataset, positive_train_dataset])\n",
    "    validate_dataset = torch.utils.data.ConcatDataset([negative_validate_dataset, positive_validate_dataset])\n",
    "    \n",
    "    # print(len(train_dataset))\n",
    "    # print(len(validate_dataset))\n",
    "    # print(list(train_dataset))\n",
    "    # m = 0\n",
    "    # for data, label in validate_dataset:\n",
    "    #     m += 1\n",
    "    #     # print(data)\n",
    "    #     print('******************************************************************')\n",
    "    #     # print(label)\n",
    "    #     print('一共',m,'个数据')\n",
    "\n",
    "    \n",
    "    traindata_Loader = DataLoader(train_dataset, batch_size = 50, shuffle=True)\n",
    "    validatedata_Loader = DataLoader(validate_dataset, batch_size = 50, shuffle=True)\n",
    "    q = 0\n",
    "    for train, label in traindata_Loader:\n",
    "        # print(train.shape)\n",
    "        # print(label.shape)\n",
    "        q += 1\n",
    "    print('测试集一共划分为：',q,'个批次')\n",
    "    p = 0\n",
    "    for validate, label in validatedata_Loader:\n",
    "        # print(validate.shape)\n",
    "        # print(label.shape)\n",
    "        p += 1\n",
    "    print('验证集一共划分为：',p,'个批次')\n",
    "    print('***********************************************')\n",
    "    print('正式数据集加载完毕！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "pytorch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
