{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class Generate_Dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input', 'label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "# if __name__ == '__main__':\n",
    "#     dataset = Generate_Dataset('./dataset(05.09)/')\n",
    "    \n",
    "#     train_size = int(len(dataset) * 0.9)\n",
    "#     validate_size = int(len(dataset) - train_size)\n",
    "#     train_dataset, validate_dataset = torch.utils.data\\\n",
    "#                 .random_split(dataset, [train_size, validate_size])\n",
    "\n",
    "#     #print(\"读入数据个数为：\", len(top_dataset))\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "#     validate_loader = DataLoader(validate_dataset, batch_size=1, shuffle=True)\n",
    "#     t = 0\n",
    "#     for train, label in train_loader:\n",
    "#         t += 1\n",
    "#         # print(train.shape)\n",
    "#         # print(label.shape)\n",
    "#     # print('共有',t,'个训练集')\n",
    "    \n",
    "#     n = 0\n",
    "#     for validate, label in validate_loader:\n",
    "#         n += 1\n",
    "#         # print(validate.shape)\n",
    "#         # print(label.shape)\n",
    "#     # print('共有',n,'个训练集')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_path = \"./dataset(05.09)/\"\n",
    "def dataset_split(data_path, batch_size):\n",
    "    dataset = Generate_Dataset(data_path)\n",
    "    print(len(dataset))\n",
    "    train_size = int(len(dataset) * 0.9)\n",
    "    validate_size = int(len(dataset) - train_size)\n",
    "    train_dataset, validate_dataset = torch.utils.data.random_split(dataset, \\\n",
    "        [train_size, validate_size])\n",
    "    print(train_dataset.shape)\n",
    "    \n",
    "    # train_dataset = np.squeeze(train_dataset)\n",
    "\n",
    "    # for i in range(len(train_dataset)):\n",
    "    #     print(train_dataset[i].shape)\n",
    "    \n",
    "test = dataset_split(data_path, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "label_array = []\n",
    "\n",
    "dataindex = './dataset(05.09)/label/'\n",
    "for t in range(len(glob.glob(os.path.join(dataindex, '*.npy')))):\n",
    "    data = np.load(os.path.join(dataindex, str(t)+'.npy'))\n",
    "    # print(data.shape)\n",
    "    data = np.squeeze(data)\n",
    "    # print(data.shape)\n",
    "    if np.all(data == 0):\n",
    "        label_array.append(0)\n",
    "    else:\n",
    "        label_array.append(t)\n",
    "print(len(label_array))\n",
    "# print(label_array)\n",
    "\n",
    "label_array = np.array(label_array).reshape(108, 92)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(label_array)\n",
    "n = 0\n",
    "nozerolabel = []\n",
    "# print(label_array.shape[0])\n",
    "# print(label_array.shape[1])\n",
    "for i in range(label_array.shape[0]):\n",
    "    for j in range(label_array.shape[1]):\n",
    "        if label_array[i][j] == 0:\n",
    "             continue\n",
    "        else:\n",
    "            n += 1\n",
    "            nozerolabel.append(label_array[i][j])\n",
    "print(n)\n",
    "print(nozerolabel)\n",
    "print(len(nozerolabel))\n",
    "np.save('./dataset(05.09)/nozerolabelindex.npy',nozerolabel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据集整合代码__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#数据集划分整合模块\n",
    "def dataset_split(data_path, labelindex, batch_size, negative_sample_ratio):\n",
    "    \n",
    "    #计算一个batch中negative_sample和positive_sample的数量\n",
    "    negative_sample_size = int(batch_size * negative_sample_ratio)\n",
    "    print('negative_sample_size:',negative_sample_size)\n",
    "    positive_sample_size = batch_size - negative_sample_size\n",
    "    print('positive_sample_size:',positive_sample_size)\n",
    "\n",
    "    #划分存储数据集（单个batch）\n",
    "    \n",
    "\n",
    "    print('labelindex:',np.load(labelindex))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = './dataset(05.09)/'\n",
    "    labelindex = './dataset(05.09)/nozerolabelindex.npy'\n",
    "    test = dataset_split(data_path, labelindex, 50, 0.75)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__保存positive_sample__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dataindex = np.array(np.load('./dataset(05.09)/nozerolabelindex.npy'))\n",
    "print(dataindex)\n",
    "dataaddr = './dataset(05.09)/input/'\n",
    "inputsaveaddr = './dataset(positive_negative)/input/'\n",
    "\n",
    "#保存positive_sample\n",
    "for i in range(len(dataindex)):\n",
    "    # print(i)\n",
    "    index = dataindex[i]\n",
    "    print(index)\n",
    "    inputdata = np.load(os.path.join(dataaddr, str(index)+'.npy'))\n",
    "    np.save(os.path.join(inputsaveaddr,'positive_input/'+str(index)+'.npy'),inputdata)\n",
    "\n",
    "    labeladdr = dataaddr.replace('input/','label/')\n",
    "    labeldata = np.load(os.path.join(labeladdr, str(index)+'.npy'))\n",
    "    labelsaveaddr = inputsaveaddr.replace('input/','label/')\n",
    "    np.save(os.path.join(labelsaveaddr,'positive_label/'+str(index)+'.npy'),labeldata)\n",
    "\n",
    "    #保存negative_sample\n",
    "    negativesample_index = np.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__保存negative_sample__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#生成寻址标签\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "dataindex = np.arange(0,9936)\n",
    "# print(data)\n",
    "positivedataindex = np.load('./dataset(05.09)/nozerolabelindex.npy')\n",
    "print(positivedataindex)\n",
    "for i in range(positivedataindex.shape[0]):\n",
    "    temp_value = positivedataindex[i]\n",
    "    dataindex[temp_value] = 0\n",
    "\n",
    "#保存negative_sample\n",
    "inputindex = './dataset(05.09)/input/'\n",
    "inputsaveaddr = './dataset(positive_negative)/input/'\n",
    "num = 0\n",
    "for t in range(len(dataindex)):\n",
    "    temp = dataindex[t]\n",
    "    if temp == 0:\n",
    "        continue\n",
    "    else:\n",
    "        inputdata = np.load(os.path.join(inputindex,str(temp)+'.npy'))\n",
    "        num += 1\n",
    "        np.save(os.path.join(inputsaveaddr,'negative_input/'+str(temp)+'.npy'), inputdata)\n",
    "        labelindex = inputindex.replace('input/','label/')\n",
    "        labeldata = np.load(os.path.join(labelindex,str(temp)+'.npy'))\n",
    "        labelsaveaddr = inputsaveaddr.replace('input/', 'label/')\n",
    "        np.save(os.path.join(labelsaveaddr, 'negative_label/'+str(temp)+'.npy'), labeldata)\n",
    "\n",
    "print('一共：',num, '个negative_sample!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IOU指标实现__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [0 2 1]]\n",
      "[[2 1 0]\n",
      " [1 0 1]]\n",
      "cm [[0 1 1]\n",
      " [0 2 0]\n",
      " [2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "y_true = np.array([[0, 1, 2],[0, 2, 1]])\n",
    "print(y_true)\n",
    "y_pred = np.array([[2, 1, 0],[1, 0, 1]])\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "def cal_cm(y_true, y_pred):\n",
    "    y_true = y_true.reshape(1, -1).squeeze()\n",
    "    y_pred = y_pred.reshape(1, -1).squeeze()\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    return cm\n",
    "\n",
    "cm = cal_cm(y_true, y_pred)\n",
    "print('cm',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp tensor([[0, 0, 0],\n",
      "        [0, 1, 1],\n",
      "        [0, 2, 0]])\n",
      "weight [0.5025125628140701, 0.2537814064007222, 0.2040401995838798, 0]\n",
      "weight_dice [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "temp = torch.randint(0, 3, (3, 3))\n",
    "print('temp',temp) \n",
    "confusion = [2, 4, 5, 1] \n",
    "\n",
    "def cb_loss(temp, beta, confusion):\n",
    "\n",
    "    confusion_ = [torch.sum(temp == i) for i in range(4)]\n",
    "\n",
    "    weight = []\n",
    "    weight_dice = []\n",
    "    for i, n in zip(confusion_, confusion):\n",
    "        if i == 0:\n",
    "            weight.append(0)\n",
    "            weight_dice.append(1)\n",
    "        else:\n",
    "            weight_dice.append(1)\n",
    "            weight.append(((1.0 - beta) / (1.0 - math.pow(beta, n))))\n",
    "    return weight, weight_dice\n",
    "\n",
    "\n",
    "weight, weight_dice = cb_loss(temp, 0.99, confusion)\n",
    "print('weight',weight)\n",
    "print('weight_dice',weight_dice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "pytorch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
