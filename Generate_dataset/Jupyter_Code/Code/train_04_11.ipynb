{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据导入__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class Generate_Dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'negative_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/negative_input', 'label/negative_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "if __name__ == '__main__':\n",
    "    dataset = Generate_Dataset('./dataset(positive_negative)/input/')\n",
    "    \n",
    "    train_size = int(len(dataset) * 0.9)\n",
    "    validate_size = int(len(dataset) - train_size)\n",
    "    train_dataset, validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset, [train_size, validate_size])\n",
    "\n",
    "    #print(\"读入数据个数为：\", len(top_dataset))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "    validate_loader = DataLoader(validate_dataset, batch_size=50, shuffle=True)\n",
    "    t = 0\n",
    "    for train, label in train_loader:\n",
    "        t += 1\n",
    "        print(train.shape)\n",
    "        print(label.shape)\n",
    "    print('共有',t,'个训练集')\n",
    "    \n",
    "    n = 0\n",
    "    for validate, label in validate_loader:\n",
    "        n += 1\n",
    "        print(validate.shape)\n",
    "        print(label.shape)\n",
    "    print('共有',n,'个训练集')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__数据集加载最新代码（negative_sample:positive_sample=7:3版本）__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************读入negative_sample***********************\n",
      "negative_train_size: 6733\n",
      "negative_validate_size: 2886\n",
      "读入数据个数为： 6733\n",
      "*****************************读入完毕！****************************\n",
      "\n",
      "\n",
      "************************读入positive_sample***********************\n",
      "positive_train_size: 222\n",
      "positive_validate_size: 95\n",
      "读入数据个数为： 222\n",
      "*****************************读入完毕！****************************\n",
      "\n",
      "\n",
      "*************************开始加载正式数据集*************************\n",
      "测试集一共划分为： 140 个批次\n",
      "验证集一共划分为： 60 个批次\n",
      "*************************正式数据集加载完毕*************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class Generate_Dataset1(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'negative_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/negative_input', 'label/negative_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "class Generate_Dataset2(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_path = glob.glob\\\n",
    "            (os.path.join(path,'positive_input/*.npy'))  #读取data文件夹下所有.npy格式文件\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data_path[index]\n",
    "        # print(data_path)\n",
    "        data = np.load(data_path)      #读取输入数据\n",
    "        tensor_data = torch.from_numpy(data)\n",
    "        \n",
    "        label_path = data_path.replace('input/positive_input', 'label/positive_label')\n",
    "        label = np.load(label_path)    #读取标签数据\n",
    "        tensor_label = torch.from_numpy(label)\n",
    "\n",
    "        return tensor_data, tensor_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "\n",
    "#测试载入数据程序\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    #************************************************************************************************\n",
    "    #读取并载入negative_dataset\n",
    "    print('************************读入negative_sample***********************')\n",
    "    dataset1 = Generate_Dataset1('../Data/dataset(positive_negative)/input/')\n",
    "    negative_train_size = int(len(dataset1) * 0.7)\n",
    "    print('negative_train_size:', negative_train_size)\n",
    "    negative_validate_size = int(len(dataset1) - negative_train_size)\n",
    "    print('negative_validate_size:', negative_validate_size)\n",
    "\n",
    "    negative_train_dataset, negative_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset1, [negative_train_size, negative_validate_size])\n",
    "\n",
    "    print(\"读入数据个数为：\", len(negative_train_dataset))\n",
    "    print('*****************************读入完毕！****************************')\n",
    "    print('\\n')\n",
    "    train_loader = DataLoader(negative_train_dataset, batch_size=1, shuffle=True)\n",
    "    validate_loader = DataLoader(negative_validate_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # t = 0\n",
    "    # for negative_train, negative_label in train_loader:\n",
    "    #     t += 1\n",
    "    #     print(negative_train.shape)\n",
    "    #     print(negative_label.shape)\n",
    "    # print('共有',t,'个训练集')\n",
    "    \n",
    "    # n = 0\n",
    "    # for negative_validate, negative_label in validate_loader:\n",
    "    #     n += 1\n",
    "    #     print(negative_validate.shape)\n",
    "    #     print(negative_label.shape)\n",
    "    # print('共有',n,'个训练集')\n",
    "\n",
    "\n",
    "\n",
    "    # print('******************************分割线******************************')\n",
    "\n",
    "\n",
    "    #************************************************************************************************\n",
    "    #读取并载入positive_dataset\n",
    "    print('************************读入positive_sample***********************')\n",
    "    dataset2 = Generate_Dataset2('../Data/dataset(positive_negative)/input/')\n",
    "    positive_train_size = int(len(dataset2) * 0.7) + 1\n",
    "    print('positive_train_size:',positive_train_size)\n",
    "    positive_validate_size = int(len(dataset2) - positive_train_size)\n",
    "    print('positive_validate_size:', positive_validate_size)\n",
    "    positive_train_dataset, positive_validate_dataset = torch.utils.data\\\n",
    "                .random_split(dataset2, [positive_train_size, positive_validate_size])\n",
    "\n",
    "    print(\"读入数据个数为：\", len(positive_train_dataset))\n",
    "    print('*****************************读入完毕！****************************')\n",
    "    train_loader = DataLoader(positive_train_dataset, batch_size=1, shuffle=True)\n",
    "    validate_loader = DataLoader(positive_validate_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # t = 0\n",
    "    # for positive_train, positive_label in train_loader:\n",
    "    #     t += 1\n",
    "    #     # print(positive_train.shape)\n",
    "    #     # print(positive_label.shape)\n",
    "    # # print('共有',t,'个训练集')\n",
    "    \n",
    "    # n = 0\n",
    "    # for positive_validate, positive_label in validate_loader:\n",
    "    #     n += 1\n",
    "    #     # print(positive_validate.shape)\n",
    "    #     # print(positive_label.shape)\n",
    "    # # print('共有',n,'个训练集')\n",
    "    print('\\n')\n",
    "    print('*************************开始加载正式数据集*************************')\n",
    "    train_dataset = torch.utils.data.ConcatDataset([negative_train_dataset, positive_train_dataset])\n",
    "    validate_dataset = torch.utils.data.ConcatDataset([negative_validate_dataset, positive_validate_dataset])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(len(train_dataset))\n",
    "    # print(len(validate_dataset))\n",
    "    # print(list(train_dataset))\n",
    "    # m = 0\n",
    "    # for data, label in validate_dataset:\n",
    "    #     m += 1\n",
    "    #     # print(data)\n",
    "    #     print('******************************************************************')\n",
    "    #     # print(label)\n",
    "    #     print('一共',m,'个数据')\n",
    "\n",
    "    \n",
    "    traindata_Loader = DataLoader(train_dataset, batch_size = 50, shuffle=True)\n",
    "    validatedata_Loader = DataLoader(validate_dataset, batch_size = 50, shuffle=True)\n",
    "    q = 0\n",
    "    for train, label in traindata_Loader:\n",
    "        # print(train.shape)\n",
    "        # print(label.shape)\n",
    "        q += 1\n",
    "    print('测试集一共划分为：',q,'个批次')\n",
    "    p = 0\n",
    "    for validate, label in validatedata_Loader:\n",
    "        # print(validate.shape)\n",
    "        # print(label.shape)\n",
    "        p += 1\n",
    "    print('验证集一共划分为：',p,'个批次')\n",
    "    print('*************************正式数据集加载完毕*************************')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__unet3d_parts__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3d_init(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv3d_init, self).__init__()\n",
    "        self.double_conv3d_init = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels=32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, out_channels, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.double_conv3d_init(input)\n",
    "\n",
    "\n",
    "class DoubleConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv3d, self).__init__()\n",
    "        self.double_conv3d = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, in_channels, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.double_conv3d(input)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down,self).__init__()\n",
    "        self.maxpool_conv3d = nn.Sequential(\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2, padding=0),\n",
    "            DoubleConv3d(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.maxpool_conv3d(input)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up3d = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        self.conv = DoubleConv3d(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, input, x):  #x是接收的从encoder传过来的融合数据\n",
    "       \n",
    "        x1 = self.up3d(input)     #x1是上采样后的数据\n",
    "\n",
    "        #特征融合部分\n",
    "        diffY = torch.tensor(x1.size()[3] - x.size()[3])\n",
    "        diffX = torch.tensor(x1.size()[4] - x.size()[4])\n",
    "        diffZ = torch.tensor(x1.size()[2] - x.size()[2])\n",
    "        \n",
    "        x3 = F.pad(x, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2))\n",
    "        #print('x3',x3.shape)\n",
    "        output = torch.cat([x1, x3], dim = 1)\n",
    "        return self.conv(output)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                    nn.Conv3d(in_channels, out_channels, kernel_size=(1, 1, 1)))\n",
    "    def forward(self, input):\n",
    "        return self.conv1(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__unet3d_model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self,in_channels, n_classes):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        #Encoder\n",
    "        self.inc = DoubleConv3d_init(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "\n",
    "        #Decoder\n",
    "        self.up1 = Up(768, 256)\n",
    "        self.up2 = Up(384, 128)\n",
    "        self.up3 = Up(192, 64)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out1 = self.inc(input)\n",
    "        print('out1.shape:',out1.shape)\n",
    "        out2 = self.down1(out1)\n",
    "        print('out2.shape:',out2.shape)\n",
    "        out3 = self.down2(out2)\n",
    "        print('out3.shape:',out3.shape)\n",
    "        out4 = self.down3(out3)\n",
    "        print('out4.shape:',out4.shape)\n",
    "        out5 = self.up1(out4, out3)\n",
    "        print('out5.shape:',out5.shape)\n",
    "        out6 = self.up2(out5, out2)\n",
    "        print('out6.shape:',out6.shape)\n",
    "        out7 = self.up3(out6, out1)\n",
    "        print('out7.shape:',out7.shape)\n",
    "        logits = self.outc(out7)\n",
    "        print('logits.shape:',logits.shape)\n",
    "        return logits\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     net = UNet3D(in_channels =3 ,n_classes=3)\n",
    "#     print(net)\n",
    "#     para = list(net.parameters())\n",
    "#     print('parameters:', para)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./loss')\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# def cb_loss(y_pred, y_true, temp, beta, confusion):\n",
    "\n",
    "#     #计算每个类别对应的样本数\n",
    "#     confusion_ = [torch.sum(temp == i) for i in range(2)]   #统计一张label中每个类别的像素点\n",
    "\n",
    "#     weight = []\n",
    "#     weight_dice = []\n",
    "#     for i, n in zip(confusion_, confusion):\n",
    "#         if i == 0:\n",
    "#             weight.append(0)\n",
    "#             weight_dice.append(1)\n",
    "#         else:\n",
    "#             weight_dice.append(1)\n",
    "#             weight.append(((1.0 - beta) / (1.0 - math.pow(beta, n))))\n",
    "    \n",
    "    \n",
    "#     weight = torch.FloatTensor(weight)\n",
    "#     weight_dice = torch.FloatTensor(weight_dice)\n",
    "#     weight = weight.to(y_pred.device)\n",
    "#     weight_dice = weight_dice.to(y_pred.device)\n",
    "\n",
    "\n",
    "#     criterion_train = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "#     loss = criterion_train(y_pred.float(), temp.float())\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "\n",
    "def train_net(net, device, data_path, epochs=100, batch_size=50, lr=0.001):\n",
    "    dataset = Generate_Dataset(data_path)\n",
    "    #划分训练集和验证集\n",
    "    train_size = int(len(dataset) * 0.9)\n",
    "    validate_size = int(len(dataset) - train_size)\n",
    "    train_dataset, validate_dataset = torch.utils.data.random_split(dataset, [train_size, validate_size])\n",
    "    #加载训练集和验证集\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    validate_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    #\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch:',epoch)\n",
    "        net.train()\n",
    "        \n",
    "        #训练数据集\n",
    "        for data, label in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device=device, dtype=torch.float32)\n",
    "            label = label.to(device=device, dtype=torch.float32)\n",
    "            pred = net(data)\n",
    "            temp = torch.argmax(pred, dim=1)\n",
    "            loss = cb_loss(pred, label, temp, 0.9999, cls)\n",
    "           \n",
    "            writer.add_scalar('trainloss',float(loss),epoch)\n",
    "            writer.close()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('Loss/train', loss.item())\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            torch.save(net.state_dict(), 'best_model.pth')\n",
    "        #验证集测试\n",
    "        for validatedata, validatelabel in validate_loader:\n",
    "            \n",
    "            validatedata = validatedata.to(device, dtype=torch.float32)\n",
    "            validatelabel = validatelabel.to(device, dtype=torch.float32)\n",
    "            validatepred = net(validatedata)\n",
    "            validateloss = criterion(validatepred, validatelabel)\n",
    "            writer.add_scalar('validateloss',float(validateloss),epoch)\n",
    "            print('Loss/validate', validateloss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = UNet3D(1, 1)\n",
    "    net.to(device=device)\n",
    "\n",
    "    data_path = \"../../../../Generate_dataset/Matlab_files/dataset/dataset/\"\n",
    "    train_net(net, device, data_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 选择设备，有cuda用cuda，没有就用cpu\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 加载网络，图片单通道，分类为1。\n",
    "    net = UNet3D(1, 1)\n",
    "    # 将网络拷贝到deivce中\n",
    "    net.to(device=device)\n",
    "    # 加载模型参数\n",
    "    net.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    # 测试模式\n",
    "    net.eval()\n",
    "    t = 50\n",
    "    # 保存结果地址\n",
    "    save_res_path = os.path.join('./dataset/test/'+('%d_res.png'%(t)))\n",
    "    # 转为batch为1，通道为1，大小为512*512的数组\n",
    "    data = np.load('./dataset/test/topomodel_50.npy')\n",
    "    data = data.reshape(1, 1, data.shape[0], data.shape[1], data.shape[2])\n",
    "    # 转为tensor\n",
    "    data_tensor = torch.from_numpy(data)\n",
    "    # 将tensor拷贝到device中，只用cpu就是拷贝到cpu中，用cuda就是拷贝到cuda中。\n",
    "    data_tensor = data_tensor.to(device=device, dtype=torch.float32)\n",
    "    # 预测\n",
    "    pred = net(data_tensor)\n",
    "    # 提取结果\n",
    "    pred = np.array(pred.data.cpu()[0])[0]\n",
    "    print(pred)\n",
    "    # # 处理结果\n",
    "    # for i in range(pred.shape[0]):\n",
    "    #     for j in range(pred.shape[1]):\n",
    "    #         for k in range(pred.shape[2]):\n",
    "    #             if pred[i][j][k] > 0.5:\n",
    "    #                 pred[i][j][k] = 1\n",
    "    #             else:\n",
    "    #                 pred[i][j][k] = 0\n",
    "    # 保存图片\n",
    "    np.save('./dataset/test/',pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "pytorch_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6772ea3e54c912be8400171318127616a9b64ee8b3385950b6ee172fef72396a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
